{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r\"C:\\\\Users\\\\saket\\\\Downloads\\\\audio\\\\subset\"\n",
    "JSON_PATH = \"data.json\"\n",
    "# DATASET_PATH = r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\Augumented\"\n",
    "# JSON_PATH = \"data_aug.json\"\n",
    "SAMPLES_TO_CONSIDER = 22050 # 1 sec. of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, json_path=\"\", num_mfcc=13, n_fft=2048, hop_length=512):\n",
    "\n",
    "\n",
    "    # dictionary where we'll store mapping, labels, MFCCs and filenames\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"MFCCs\": [],\n",
    "        \"files\": []\n",
    "    }\n",
    "\n",
    "    # loop through all sub-dirs\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        \n",
    "        if dirpath is not dataset_path:\n",
    "            label = dirpath.split(\"\\\\\")[-1]\n",
    "            # print(label)\n",
    "            data[\"mapping\"].append(label)\n",
    "            print(\"\\nProcessing: '{}'\".format(label))\n",
    "\n",
    "            for f in filenames:\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                signal, sample_rate = librosa.load(file_path)\n",
    "                if len(signal) >= SAMPLES_TO_CONSIDER:\n",
    "                    signal = signal[:SAMPLES_TO_CONSIDER]\n",
    "                    # print(signal)\n",
    "                    # extract MFCCs\n",
    "                    MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
    "                                                 hop_length=hop_length)\n",
    "                    # MFCCs = librosa.feature.mfcc(signal)\n",
    "\n",
    "                    # store data for analysed track\n",
    "                    data[\"MFCCs\"].append(MFCCs.T.tolist())\n",
    "                    data[\"labels\"].append(i-1)\n",
    "                    data[\"files\"].append(file_path)\n",
    "\n",
    "    # save data in json file\n",
    "    with open(json_path, \"w+\") as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'down'\n",
      "\n",
      "Processing: 'go'\n",
      "\n",
      "Processing: 'left'\n",
      "\n",
      "Processing: 'no'\n",
      "\n",
      "Processing: 'off'\n",
      "\n",
      "Processing: 'on'\n",
      "\n",
      "Processing: 'right'\n",
      "\n",
      "Processing: 'stop'\n",
      "\n",
      "Processing: 'up'\n",
      "\n",
      "Processing: 'yes'\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(DATASET_PATH, JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'bed'\n",
      "\n",
      "Processing: 'bird'\n",
      "\n",
      "Processing: 'cat'\n",
      "\n",
      "Processing: 'dog'\n",
      "\n",
      "Processing: 'happy'\n",
      "\n",
      "Processing: 'house'\n",
      "\n",
      "Processing: 'one'\n",
      "\n",
      "Processing: 'tree'\n",
      "\n",
      "Processing: 'wow'\n",
      "\n",
      "Processing: 'zero'\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(DATASET_PATH, JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "model = load_model(r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\model_aug.h5\")\n",
    "from tensorflow.keras.models import load_model\n",
    "def preprocess(file_path, num_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    signal, sample_rate = librosa.load(file_path)\n",
    "    if len(signal) >=   22050:\n",
    "        signal = signal[:22050]\n",
    "        MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
    "                                        hop_length=hop_length)\n",
    "        X=np.array(MFCCs.T.tolist())\n",
    "        X_train=X[..., np.newaxis]\n",
    "        MFCC_index=model.predict(X_train)\n",
    "        print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 42, 11, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 42, 11, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 21, 6, 64)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 19, 4, 32)         18464     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 19, 4, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 2, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 9, 1, 32)          4128      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 9, 1, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 1, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                10304     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,698\n",
      "Trainable params: 34,442\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 13, 1)\n",
      "(44, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocess(r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\Audio\\Input\\file.wav\")\n",
    "preprocess(r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\Audio\\aug_left\\0e5193e6_nohash_0_aug.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess(r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\Audio\\Input\\file.wav\")\n",
    "preprocess(r\"C:\\Users\\saket\\Desktop\\xwebsite\\ML\\AI\\Audio\\aug_left\\0e5193e6_nohash_0_aug.wav\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
